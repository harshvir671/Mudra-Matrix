{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa3ef13-3e58-4e94-a961-9262e79b0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c121168-bb24-4b1e-aa0e-9673da2e539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 9670 images, 50 classes\n",
      "Balanced subset: 4932 total images\n",
      "Training images: 3945\n",
      "Test images: 987\n",
      "Number of classes: 50\n"
     ]
    }
   ],
   "source": [
    "# Set your dataset path\n",
    "dataset_path = r\"C:\\Users\\harta\\Desktop\\mudra_dataset\"  # Replace YourName with your actual username\n",
    "\n",
    "# Define transformations\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load full dataset\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)\n",
    "print(f\"Full dataset: {len(full_dataset)} images, {len(full_dataset.classes)} classes\")\n",
    "\n",
    "# Create balanced subset with max 100 images per class\n",
    "def create_balanced_subset(dataset, max_samples_per_class=100):\n",
    "    \"\"\"Create a balanced subset with limited samples per class\"\"\"\n",
    "    class_indices = {}\n",
    "    \n",
    "    # Group indices by class\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        if label not in class_indices:\n",
    "            class_indices[label] = []\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Select limited samples per class\n",
    "    selected_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        if len(indices) > max_samples_per_class:\n",
    "            # Randomly select max_samples_per_class indices\n",
    "            selected_indices.extend(np.random.choice(indices, max_samples_per_class, replace=False))\n",
    "        else:\n",
    "            # Use all available indices\n",
    "            selected_indices.extend(indices)\n",
    "    \n",
    "    return Subset(dataset, selected_indices)\n",
    "\n",
    "# Create balanced subsets\n",
    "max_samples = 100  # Maximum images per class\n",
    "train_dataset_full = datasets.ImageFolder(root=dataset_path, transform=train_transform)\n",
    "train_subset = create_balanced_subset(train_dataset_full, max_samples)\n",
    "\n",
    "test_dataset_full = datasets.ImageFolder(root=dataset_path, transform=val_transform) \n",
    "test_subset = create_balanced_subset(test_dataset_full, max_samples)\n",
    "\n",
    "# Split the subset into train and test\n",
    "combined_indices = list(range(len(train_subset)))\n",
    "np.random.shuffle(combined_indices)\n",
    "\n",
    "train_size = int(0.8 * len(combined_indices))\n",
    "test_size = len(combined_indices) - train_size\n",
    "\n",
    "train_indices = combined_indices[:train_size]\n",
    "test_indices = combined_indices[train_size:]\n",
    "\n",
    "train_dataset = Subset(train_subset.dataset, [train_subset.indices[i] for i in train_indices])\n",
    "test_dataset = Subset(test_subset.dataset, [test_subset.indices[i] for i in test_indices])\n",
    "\n",
    "print(f\"Balanced subset: {len(train_subset)} total images\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Test images: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(full_dataset.classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af8615b-8787-442c-b163-95eccf5b9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([16, 3, 224, 224])\n",
      "Batch labels shape: torch.Size([16])\n",
      "Sample labels: tensor([11, 18, 29, 39, 43])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders with smaller batch size for faster training\n",
    "batch_size = 16  # Reduced from 32 for faster training\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test one batch\n",
    "for images, labels in train_dataloader:\n",
    "    print(f\"Batch images shape: {images.shape}\")  # Should be [16, 3, 224, 224]\n",
    "    print(f\"Batch labels shape: {labels.shape}\")  # Should be [16]\n",
    "    print(f\"Sample labels: {labels[:5]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c0a8eaf-9d3b-470c-a77c-5fb7b3764a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harta\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\harta\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\harta/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:19<00:00, 2.46MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model setup for 50 classes\n",
      "Using ResNet18 for faster training\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use a smaller model for faster training\n",
    "model = models.resnet18(pretrained=True)  # ResNet18 instead of ResNet50\n",
    "\n",
    "# Replace the final layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(full_dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)  # Faster learning rate decay\n",
    "\n",
    "print(f\"Model setup for {len(full_dataset.classes)} classes\")\n",
    "print(\"Using ResNet18 for faster training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c556a7-5b0d-4bb8-9073-007ecba5da20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fast training with balanced subset...\n",
      "Epoch 1, Batch 0, Loss: 3.9565\n",
      "Epoch 1, Batch 20, Loss: 3.3114\n",
      "Epoch 1, Batch 40, Loss: 1.8960\n",
      "Epoch 1, Batch 60, Loss: 1.7371\n",
      "Epoch 1, Batch 80, Loss: 1.8553\n",
      "Epoch 1, Batch 100, Loss: 1.0713\n",
      "Epoch 1, Batch 120, Loss: 1.4167\n",
      "Epoch 1, Batch 140, Loss: 1.3502\n",
      "Epoch 1, Batch 160, Loss: 0.8192\n",
      "Epoch 1, Batch 180, Loss: 0.9228\n",
      "Epoch 1, Batch 200, Loss: 0.6842\n",
      "Epoch 1, Batch 220, Loss: 0.9219\n",
      "Epoch 1, Batch 240, Loss: 0.7570\n",
      "Epoch [1/8], Train Loss: 1.5728, Test Accuracy: 68.59%\n",
      "Epoch 2, Batch 0, Loss: 0.6057\n",
      "Epoch 2, Batch 20, Loss: 1.1536\n",
      "Epoch 2, Batch 40, Loss: 0.6600\n",
      "Epoch 2, Batch 60, Loss: 0.6255\n",
      "Epoch 2, Batch 80, Loss: 1.5663\n",
      "Epoch 2, Batch 100, Loss: 0.5905\n",
      "Epoch 2, Batch 120, Loss: 0.5319\n",
      "Epoch 2, Batch 140, Loss: 0.5826\n",
      "Epoch 2, Batch 160, Loss: 0.4021\n",
      "Epoch 2, Batch 180, Loss: 0.7799\n",
      "Epoch 2, Batch 200, Loss: 0.4436\n",
      "Epoch 2, Batch 220, Loss: 0.6693\n",
      "Epoch 2, Batch 240, Loss: 1.4037\n",
      "Epoch [2/8], Train Loss: 0.6758, Test Accuracy: 73.96%\n",
      "Epoch 3, Batch 0, Loss: 0.3250\n",
      "Epoch 3, Batch 20, Loss: 0.2114\n",
      "Epoch 3, Batch 40, Loss: 0.6318\n",
      "Epoch 3, Batch 60, Loss: 0.4621\n",
      "Epoch 3, Batch 80, Loss: 0.5666\n",
      "Epoch 3, Batch 100, Loss: 0.3617\n",
      "Epoch 3, Batch 120, Loss: 0.6889\n",
      "Epoch 3, Batch 140, Loss: 0.5032\n",
      "Epoch 3, Batch 160, Loss: 0.7045\n",
      "Epoch 3, Batch 180, Loss: 0.6442\n",
      "Epoch 3, Batch 200, Loss: 0.5207\n",
      "Epoch 3, Batch 220, Loss: 0.5234\n",
      "Epoch 3, Batch 240, Loss: 0.9899\n",
      "Epoch [3/8], Train Loss: 0.5150, Test Accuracy: 87.44%\n",
      "Epoch 4, Batch 0, Loss: 0.1733\n",
      "Epoch 4, Batch 20, Loss: 0.1308\n",
      "Epoch 4, Batch 40, Loss: 0.2308\n",
      "Epoch 4, Batch 60, Loss: 0.2477\n",
      "Epoch 4, Batch 80, Loss: 0.1972\n",
      "Epoch 4, Batch 100, Loss: 0.3041\n",
      "Epoch 4, Batch 120, Loss: 0.3696\n",
      "Epoch 4, Batch 140, Loss: 0.1349\n",
      "Epoch 4, Batch 160, Loss: 0.1043\n",
      "Epoch 4, Batch 180, Loss: 0.0338\n",
      "Epoch 4, Batch 200, Loss: 0.0723\n",
      "Epoch 4, Batch 220, Loss: 0.2015\n",
      "Epoch 4, Batch 240, Loss: 0.1932\n",
      "Epoch [4/8], Train Loss: 0.2219, Test Accuracy: 96.15%\n",
      "Epoch 5, Batch 0, Loss: 0.4004\n",
      "Epoch 5, Batch 20, Loss: 0.0550\n",
      "Epoch 5, Batch 40, Loss: 0.4024\n",
      "Epoch 5, Batch 60, Loss: 0.3235\n",
      "Epoch 5, Batch 80, Loss: 0.1366\n",
      "Epoch 5, Batch 100, Loss: 0.2410\n",
      "Epoch 5, Batch 120, Loss: 0.1297\n",
      "Epoch 5, Batch 140, Loss: 0.1571\n",
      "Epoch 5, Batch 160, Loss: 0.1353\n",
      "Epoch 5, Batch 180, Loss: 0.1388\n",
      "Epoch 5, Batch 200, Loss: 0.1782\n",
      "Epoch 5, Batch 220, Loss: 0.0672\n",
      "Epoch 5, Batch 240, Loss: 0.0659\n",
      "Epoch [5/8], Train Loss: 0.1599, Test Accuracy: 95.54%\n",
      "Epoch 6, Batch 0, Loss: 0.2251\n",
      "Epoch 6, Batch 20, Loss: 0.1860\n",
      "Epoch 6, Batch 40, Loss: 0.0885\n",
      "Epoch 6, Batch 60, Loss: 0.0909\n",
      "Epoch 6, Batch 80, Loss: 0.1803\n",
      "Epoch 6, Batch 100, Loss: 0.0421\n",
      "Epoch 6, Batch 120, Loss: 0.0979\n",
      "Epoch 6, Batch 140, Loss: 0.1428\n",
      "Epoch 6, Batch 160, Loss: 0.3613\n",
      "Epoch 6, Batch 180, Loss: 0.3131\n",
      "Epoch 6, Batch 200, Loss: 0.1643\n",
      "Epoch 6, Batch 220, Loss: 0.1476\n",
      "Epoch 6, Batch 240, Loss: 0.2483\n",
      "Epoch [6/8], Train Loss: 0.1596, Test Accuracy: 95.04%\n",
      "Epoch 7, Batch 0, Loss: 0.1128\n",
      "Epoch 7, Batch 20, Loss: 0.0790\n",
      "Epoch 7, Batch 40, Loss: 0.0796\n",
      "Epoch 7, Batch 60, Loss: 0.0508\n",
      "Epoch 7, Batch 80, Loss: 0.0329\n",
      "Epoch 7, Batch 100, Loss: 0.0268\n",
      "Epoch 7, Batch 120, Loss: 0.0368\n",
      "Epoch 7, Batch 140, Loss: 0.0732\n",
      "Epoch 7, Batch 160, Loss: 0.0790\n",
      "Epoch 7, Batch 180, Loss: 0.0148\n",
      "Epoch 7, Batch 200, Loss: 0.0492\n",
      "Epoch 7, Batch 220, Loss: 0.1128\n",
      "Epoch 7, Batch 240, Loss: 0.0916\n",
      "Epoch [7/8], Train Loss: 0.0833, Test Accuracy: 98.48%\n",
      "Epoch 8, Batch 0, Loss: 0.0333\n",
      "Epoch 8, Batch 20, Loss: 0.0136\n",
      "Epoch 8, Batch 40, Loss: 0.0979\n",
      "Epoch 8, Batch 60, Loss: 0.0205\n",
      "Epoch 8, Batch 80, Loss: 0.0044\n",
      "Epoch 8, Batch 100, Loss: 0.0271\n",
      "Epoch 8, Batch 120, Loss: 0.0364\n",
      "Epoch 8, Batch 140, Loss: 0.0152\n",
      "Epoch 8, Batch 160, Loss: 0.0650\n",
      "Epoch 8, Batch 180, Loss: 0.0169\n",
      "Epoch 8, Batch 200, Loss: 0.0335\n",
      "Epoch 8, Batch 220, Loss: 0.0247\n",
      "Epoch 8, Batch 240, Loss: 0.2499\n",
      "Epoch [8/8], Train Loss: 0.0694, Test Accuracy: 97.67%\n",
      "Fast training completed!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8  # Reduced from 15 for faster training\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"Starting fast training with balanced subset...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 20 batches\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print('Fast training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba508bd9-d0a2-4103-91a7-105b91f92733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint loaded successfully!\n",
      "Training had 8 epochs\n",
      "Best validation accuracy: 98.48%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load('fast_mudra_classifier.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# Extract information\n",
    "class_to_idx = checkpoint['class_to_idx']\n",
    "idx_to_class = checkpoint['idx_to_class']\n",
    "train_losses = checkpoint['train_losses']\n",
    "val_accuracies = checkpoint['val_accuracies']\n",
    "\n",
    "print(\"Model checkpoint loaded successfully!\")\n",
    "print(f\"Training had {len(train_losses)} epochs\")\n",
    "print(f\"Best validation accuracy: {max(val_accuracies):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce80987-6346-4eaa-902a-8e528236f994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harta\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\harta\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model architecture: resnet18\n",
      "Number of classes: 50\n"
     ]
    }
   ],
   "source": [
    "# Initialize model architecture\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_to_idx))\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model architecture: {checkpoint['model_architecture']}\")\n",
    "print(f\"Number of classes: {checkpoint['num_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07edabb7-b37f-480b-9534-81ea72388be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataloader created with 500 images\n"
     ]
    }
   ],
   "source": [
    "# Define transformations (same as during training)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Reload test dataset\n",
    "dataset_path = r\"C:\\Users\\harta\\Desktop\\mudra_dataset\"\n",
    "test_dataset_full = datasets.ImageFolder(root=dataset_path, transform=val_transform)\n",
    "\n",
    "# We need to recreate the test subset exactly as during training\n",
    "# Since we can't exactly reproduce the random split, let's use a simple approach\n",
    "# Alternatively, you can load the full test set for evaluation\n",
    "\n",
    "# For now, let's use a smaller test set for evaluation\n",
    "test_subset = torch.utils.data.Subset(test_dataset_full, range(500))  # Use first 500 images\n",
    "test_dataloader = torch.utils.data.DataLoader(test_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Test dataloader created with {len(test_subset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e010f2e6-3a6f-4c3a-b3e0-7e1a6c83ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Final Test Accuracy: 96.80%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Classification Report:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 10, does not match size of target_names, 50. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Classification report\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š Classification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(all_true_labels, all_predictions, \n\u001b[0;32m     39\u001b[0m                            target_names\u001b[38;5;241m=\u001b[39m[idx_to_class[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(idx_to_class))],\n\u001b[0;32m     40\u001b[0m                            zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2648\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2642\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2643\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2644\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2645\u001b[0m             )\n\u001b[0;32m   2646\u001b[0m         )\n\u001b[0;32m   2647\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2648\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2652\u001b[0m         )\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2654\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 10, does not match size of target_names, 50. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Run evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Final accuracy\n",
    "final_accuracy = 100 * np.mean(np.array(all_predictions) == np.array(all_true_labels))\n",
    "print(f\"ðŸŽ¯ Final Test Accuracy: {final_accuracy:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(15, 12))\n",
    "cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "sns.heatmap(cm, annot=False, cmap='Blues', xticklabels=False, yticklabels=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(all_true_labels, all_predictions, \n",
    "                           target_names=[idx_to_class[i] for i in range(len(idx_to_class))],\n",
    "                           zero_division=0, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63cee5c1-1101-48af-8c20-19a5843ad746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mudra(image_path, model, transform, idx_to_class, device='cpu'):\n",
    "    \"\"\"Predict mudra from an image file - simple version\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    predicted_class = idx_to_class[predicted.item()]\n",
    "    confidence_score = confidence.item()\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': predicted_class,\n",
    "        'confidence': confidence_score,\n",
    "        'all_probabilities': probabilities.cpu().numpy()[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98fe0b86-58b5-44c7-afc8-35adab200120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Results:\n",
      "Predicted class: Suchi(1)\n",
      "Confidence: 55.21%\n"
     ]
    }
   ],
   "source": [
    "# Get the full path to your desktop image\n",
    "# Replace 'your_photo.jpg' with your actual filename\n",
    "image_path = r\"C:\\Users\\harta\\Desktop\\finger-Photoroom.png\"\n",
    "\n",
    "# Test the prediction\n",
    "prediction = predict_mudra(image_path, model, val_transform, idx_to_class, device)\n",
    "print(\"Prediction Results:\")\n",
    "print(f\"Predicted class: {prediction['predicted_class']}\")\n",
    "print(f\"Confidence: {prediction['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdee2688-e21f-4049-9460-f99644d5b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'fast_mudra_classifier.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_to_idx': class_to_idx,\n",
    "    'idx_to_class': {v: k for k, v in class_to_idx.items()},\n",
    "    'train_losses': train_losses,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'model_architecture': 'resnet18',\n",
    "    'num_classes': 50\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'fast_mudra_classifier.pth')\n",
    "print(\"Model saved as 'fast_mudra_classifier.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75295ab7-8274-4637-89d9-0fcf0aaf5e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
